# Movies-ETL

## Overview
The purpose of this Challenge is to help Britta to create an automated pipeline that takes in new data, clean, transform and load the data into database. We have created a function that takes in three files Wikipedia data, Kaggle Data and the MovieLens rating data and performs ETL process by adding the data to PostgrSQL database.

## Resources
- Python
- Anaconda
- Jupyter Notebook
- PostgreSQL / PgAdmin

## Summary
In this challenge we were able to practice more python funtions, string manipulations and regex. This also provided experience on how to find bad/corrupted information in a large data source and clean the same. A new thing I learnt in this challenge was there is a limit of 100MB data that can be pushed to Github
